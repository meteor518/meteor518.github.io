---
layout: post
title:  "类别不平衡解决策略"

---

# 类别不平衡
是指分类任务中不同类别的训练样本数目差别很大的情况。一般情况下，正例样本较少，反例样本较多。

### 基本策略----**再缩放**：

以线性分类器为例，使用y=w^Tx+b对新样本x进行分类时，事实上是用预测值与阈值比较，通常y>0.5判为正例，否则反例。y实际上表达的是正例的可能性，几率y/(1-y)则反映了正例与反例可能性之比。若阈值为0.5，表明分类器认为正反例的可能性相同，即：
![]({{site.url}}/images/CNN/unbalance1.png)
当正反例数目不同时，令m^+表示正例，m^-表示反例，则观测几率是m^+/m^-，因此判断规则应如下：
![]({{site.url}}/images/CNN/unbalance2.png)
但分类器的决策规则是按公式1进行的，所以要对预测值进行调整：
![]({{site.url}}/images/CNN/unbalance3.png)

再缩放的思想简单，但实际操作却不平凡。一般是基于原始数据集进行训练学习，当用训练好的分类器预测时，将上述调整策略嵌入到决策过程中，称为`“阈值移动”`。

### 下采样
一般对反例(数目多)进行下采样，即去除一些反例的样本，使两类数目接近。

代表算法：**EasyEnsemble**(一种集成学习机制)

思想如下：


### 上采样
对正例(数目少)进行上采样，或者叫“过采样”，即增加一些正例，使两类数目接近。

代表算法：**SMOTE**

思想如下：