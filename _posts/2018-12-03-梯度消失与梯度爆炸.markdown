---
layout: post
title:  "梯度消失与梯度爆炸"

---

## 产生的原因

### 神经网络中梯度不稳定的根本原因

深度神经网络训练的时候，采用的是`反向传播`方式，该方式使用`链式求导`，计算每层梯度的时候会涉及一些连乘操作，前层上的梯度的计算来自于后层上梯度的乘积，因此如果网络过深，就容易出现不稳定。

如果连乘的因子大部分小于1，最后乘积的结果可能趋于0，也就是梯度消失，后面的网络层的参数不发生变化.

如果连乘的因子大部分大于1，最后乘积可能趋于无穷，这就是梯度爆炸。


### 梯度消失经常出现出以下两种情况：

一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。

（梯度消失情况出现时，靠近输出层的hidden layer n 的权值更新相对正常，但是靠近输入层的hidden layer 1 的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。）

### 梯度爆炸一般出现在: 
深层网络和权值初始化值太大的情况下，前面层会比后面层变化的更快，就会导致权值越来越大。

(sigmoid的导数的最大值为1/4, 通常来说当激活函数是sigmoid时,梯度消失比梯度爆炸更容易发生。)

## 梯度消失、爆炸的解决方案

### 1. 预训练加微调

此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

### 2. 梯度剪切

该方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

（注：在WGAN中也有梯度剪切限制操作，但是和这个是不一样的，WGAN限制梯度更新信息是为了保证lipchitz条件。）

### 3. 正则项

另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是L1正则，和L2正则。

正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式:
![RL]({{site.url}}/images/RL.png)
其中，a是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

一般情况下，在深层网络中，梯度消失出现的情况更多一些。

### 4. 选择合适的激活函数：
如选择relu、leakrelu、elu等激活函数

### 5. BN层

具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化保证网络的稳定性。

此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：

正向传播中![eq1]({{site.url}}/images/CNN/td_eq1.png)
那么反向传播中![eq2]({{site.url}}/images/CNN/td_eq2.png)
反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。

### 6. 残差结构

参见论文[Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
