---
layout: post
title:  "卷积神经网络(CNN)介绍08-Dropout层"

---

# Dropout

详情见论文[Improving neural networks by preventing co-adaptation of feature Detectors](https://arxiv.org/abs/1207.0580)

### Dropout原理

![Dropout]({{site.url}}/images/CNN/dropout.png)

如上图左，为没有Dropout的普通2层全连接结构，记为 r=a(Wv)，其中a为激活函数。

如上图右，为在第2层全连接后添加Dropout层的示意图。即在模型训练时随机让网络的某些节点不工作输出置0），其它过程不变。

对于Dropout这样的操作为何可以`防止训练过拟合`，原作者也没有给出数学证明，只是有一些直观的理解或者说猜想。下面有几个我认为比较靠谱的解释：

1. 由于随机的让一些节点不工作了，因此可以避免某些特征只在固定组合下才生效，有意识地让网络去学习一些普遍的共性（而不是某些训练样本的一些特性）
2. Bagging方法通过对训练数据有放回的采样来训练多个模型。而Dropout的随机意味着每次训练时只训练了一部分，而且其中大部分参数还是共享的，因此和Bagging有点相似。因此，Dropout可以看做训练了多个模型，实际使用时采用了模型平均作为输出

### 训练过程
训练的时候，我们通常设定一个dropout ratio = p,即每一个输出节点以概率 p 置0(不工作)。假设每一个输出都是相互独立的，每个输出都服从二项伯努利分布B(1-p)，则大约认为训练时 只使用了 (1-p)比例的输出。

### 测试过程
测试的时候，最直接的方法就是保留Dropout层的同时，将一张图片重复测试M次，取M次结果的平均作为最终结果。假如有N个节点，则可能的情况为R=2^N,如果M远小于R，则显然平均效果不好；如果M≈N，那么计算量就太大了。因此作者做了一个近似：可以直接去掉Dropout层，将所有输出 都使用 起来，为此需要将尺度对齐，即比例缩小输出 r=r×(1-p)。 如下面公式所示：
![eq]({{site.url}}/images/CNN/dropout_eq.png)
特别的， 为了使用方便，我们不在测试时再缩小输出，而在训练时直接将输出放大1/(1-p)倍。

预测过程的图示如下：
![test_dropout]({{site.url}}/images/CNN/dropout2.png)