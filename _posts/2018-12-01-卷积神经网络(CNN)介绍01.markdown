---
layout: post
title:  "卷积神经网络(CNN)介绍01"

---

# 卷积神经网络(CNN)

CNN ---- Convolutional Neural Networks


### 1.介绍

卷积神经网络是一种多层神经网络，擅长处理图像特别是大图像的相关机器学习问题。

卷积网络通过一系列方法，成功将数据量庞大的图像识别问题不断降维，最终使其能够被训练。CNN最早由Yann LeCun提出并应用在手写字体识别上（MINST）。LeCun提出的网络称为LeNet，其网络结构如下：
![LeNet]({{site.url}}/images/CNN/frame.jpg)


典型的卷积网络，由**卷积层**、**池化层**、**全连接层**组成。其中**卷积层**与**池化层**配合，组成多个`卷积组`，逐层提取特征，最终通过若干个**全连接层**完成分类。


### 2. 全连接网络与CNN的对比

全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：

1. **参数数量太多** 

考虑一个输入1000×1000像素的图片(一百万像素，现在已经不能算大图了)，输入层有1000×1000=100万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有(1000×1000+1)×100=1亿参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。

2. **没有利用像素之间的位置信息**

对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。

3. **网络层数限制**

我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。


### 3. CNN优点：

1. **局部连接** 

   每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。如下图所示：
   ![局部连接]({{site.url}}/images/CNN/jubu.jpg)

2. **权值共享** 

   一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。共享意味着：在输入图像的不同位置，所有神经元检测完全相同的特征。这说明CNN能很好的适应图片的平移不变性。

3. **下采样** 

   分为最大值下采样、均值下采样。以最大值为例，如下图所示：
   ![下采样]({{site.url}}/images/CNN/pool.png)

   均值操作：往往能保留整体数据的特征，能凸出背景的信息；

   最大值操作：能更好的保留纹理上的特征。



   **作用：**

   1）可以使用Pooling来减少每层的样本数，进一步减少参数数量；

   2）同时还可以提升模型的鲁棒性，降低过拟合(个人理解的是，一般特征维度越高，计算量越大，越易发生过拟合，所以减小输出的大小可以降低)。

   3）个人理解：池化相当于把像素分辨率降低，抽象特征更为具体。

**为何卷积或池化堆叠的方式可行？**

因为图像具有一种“静态性”的属性，也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。另外为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如求某个区域上某个特定特征的均值(或最大值)来代表这个区域的特征，也就是尽可能保留重要的参数，去掉大量不重要的参数，从而可能达到更好的学习效果。

**使用多层卷积的目的:**

一层卷积学到的特征往往是局部的，层数越高，学到的特征越全局化。(我理解的是：这是由于卷积或者池化层的存在，层数越高，使图片越小。则相同大小的卷积核所能覆盖图像的范围越大，学到的特征越来越全局化。)

