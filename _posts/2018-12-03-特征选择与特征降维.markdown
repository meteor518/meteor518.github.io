---
layout: post
title:  "特征选择与数据降维(特征提取)"

---

# 降维

## 降维的方法：
1. 结合专业知识剔除或合并类别
2. 通过数据概要来发现变量间的信息重叠(并剔除或合并类别)
3. 对数据进行转换，例如将分类型变量转换为数值型变量
4. 使用如主成分分析(PCA)这样的自动降维技术来创建一系列新的变量(原变量的加权平均)。这些变量互不相关，并且其中很小的一个子集就包含了原始数据中很大一部分信息(因此我们可以只使用新变量集的一个子集来实现降维)。

可以利用一些数据挖掘的方法：如回归模型、分类和回归树等，这些方法可以用于剔除冗余变量，以及合并分类型变量中的相似类别。

# 特征选择与数据降维(特征提取)区别与联系

和feature selection不同之处在于feature extraction是，但是feature selection则只是在原有特征上进行筛选。

1. `特征提取`是在原有特征基础之上去创造凝练出一些新的特征出来，好比从杂乱无章的世界中，去到更高层的世界去俯瞰原始世界，你会发现很多杂乱无章的物理现象中背后暗含的道理是想通的，这时候你想用一个更加普世的观点和理论去解释原先的理论，这个是特征提取要做的事情。

	Feature extraction有多种方法，包括PCA,LDA,LSA等等，相关算法则更多，pLSA,LDA,ICA,FA,UV-Decomposition,LFM,SVD等等。这里面有一个共同的算法，那就是鼎鼎大名的SVD。

2. 而你仍呆在原始世界中，只是想对现有的“取其精华，去其糟粕”，这个是所谓`特征选择`。只是对现有进行筛选。

3. 特征提取和特征选择统称为`降维`。（Dimension Reduction）






